# Implementaci√≥n TreeDRNet

> **Tiempo de presentaci√≥n**: 5 minutos  
> **Objetivo**: Explicar detalles t√©cnicos de la implementaci√≥n

## üìë √çndice de Contenidos (Timing Sugerido)

1. **Arquitectura de M√≥dulos** (1 min) - DResBlock ‚Üí TreeDRNet
2. **Estructura del √Årbol** (2 min) - Forward pass y bug corregido
3. **Pipeline de Datos** (1 min) - Carga, normalizaci√≥n, ventanas
4. **Optimizaci√≥n** (1 min) - Entrenamiento, scheduler, checkpointing

---

## üèóÔ∏è Arquitectura Implementada

### Jerarqu√≠a de M√≥dulos

```
TreeDRNet
‚îú‚îÄ‚îÄ Conv1D (opcional)
‚îî‚îÄ‚îÄ MultiBranchBlock
    ‚îî‚îÄ‚îÄ GatedBranch (√óK)
        ‚îú‚îÄ‚îÄ Gate Network
        ‚îî‚îÄ‚îÄ DResBlock
            ‚îú‚îÄ‚îÄ MLP
            ‚îú‚îÄ‚îÄ Backcast Head
            ‚îî‚îÄ‚îÄ Forecast Head
```

---

## üì¶ M√≥dulos Principales

### 1. DResBlock

**Responsabilidad**: Implementar el bloque doubly residual b√°sico.

```python
class DResBlock(nn.Module):
    def __init__(self, dim_in, hidden, horizonte, mlp_depth=2, dropout=0.0):
        self.mlp = Sequential(
            Linear(dim_in, hidden), ReLU(),
            [Dropout(dropout) if dropout > 0],
            Linear(hidden, hidden), ReLU(),
            ...
        )
        self.backcast = Linear(hidden, dim_in)
        self.forecast = Linear(hidden, horizonte)
```

**Forward**:
```python
h = self.mlp(x_flat)
backcast = self.backcast(h)  # Reconstrucci√≥n del input
forecast = self.forecast(h)   # Predicci√≥n del horizonte
return backcast, forecast
```

**Dimensiones**:
- Input: `(batch, L√óD)` donde L=96, D=7 para ETT
- Output backcast: `(batch, L√óD)`
- Output forecast: `(batch, H)` donde H ‚àà {24, 48, ..., 720}

---

### 2. GatedBranch

**Responsabilidad**: Implementar selecci√≥n de features + procesamiento.

```python
class GatedBranch(nn.Module):
    def __init__(self, dim_in, hidden_gate, hidden_core, horizonte, ...):
        self.gate = Sequential(
            Linear(dim_in, hidden_gate), 
            ReLU(),
            Linear(hidden_gate, dim_in), 
            Sigmoid()  # ‚Üê Importante: valores en [0,1]
        )
        self.core = DResBlock(dim_in, hidden_core, horizonte, ...)
```

**Forward**:
```python
gate_weights = self.gate(x_flat)      # (batch, L√óD)
x_selected = x_flat * gate_weights    # Element-wise
backcast, forecast = self.core(x_selected)
return backcast, forecast
```

**Intuici√≥n**: El gate aprende qu√© features son importantes para esta rama espec√≠fica.

---

### 3. MultiBranchBlock

**Responsabilidad**: Ensemble de K ramas paralelas en cada nodo.

```python
class MultiBranchBlock(nn.Module):
    def __init__(self, dim_in, horizonte, num_ramas, ...):
        self.branches = ModuleList([
            GatedBranch(...) for _ in range(num_ramas)
        ])
```

**Dos modos de forward**:

#### a) Forward regular (ensemble promediado):
```python
def forward(self, x_flat):
    bcs, fcs = [], []
    for branch in self.branches:
        bc, fc = branch(x_flat)
        bcs.append(bc)
        fcs.append(fc)
    
    avg_bc = torch.stack(bcs).mean(dim=0)  # Promedio de backcasts
    avg_fc = torch.stack(fcs).mean(dim=0)  # Promedio de forecasts
    return avg_bc, avg_fc
```

#### b) Forward todas las ramas (para √°rbol):
```python
def forward_all_branches(self, x_flat):
    bcs, fcs = [], []
    for branch in self.branches:
        bc, fc = branch(x_flat)
        bcs.append(bc)
        fcs.append(fc)
    return bcs, fcs  # Listas de K elementos
```

**Diferencia clave**: 
- `forward()`: Para predicci√≥n directa (ensemble simple)
- `forward_all_branches()`: Para crear hijos diferentes en el √°rbol

---

### 4. TreeDRNet (Modelo Principal)

**Responsabilidad**: Coordinar la estructura de √°rbol completa.

```python
class TreeDRNet(nn.Module):
    def __init__(self, entrada_dim, salida_dim, horizonte, long_ventana,
                 profundidad_arbol=3, num_ramas=2, ...):
        self.conv1x1 = Conv1d(entrada_dim, entrada_dim, 1)  # Opcional
        self.block = MultiBranchBlock(...)
        self.profundidad = profundidad_arbol
```

**Preprocesamiento**:
```python
def _prep_x(self, x):
    # x shape: (batch, L, D)
    if self.usar_conv:
        x = x.transpose(1, 2)  # ‚Üí (batch, D, L)
        x = self.conv1x1(x)    # Procesar covariables
        x = x.transpose(1, 2)  # ‚Üí (batch, L, D)
    
    x = x.reshape(batch, -1)   # ‚Üí (batch, L√óD)
    return x
```

**Forward Pass (Estructura del √Årbol)**:
```python
def forward(self, x):
    x0 = self._prep_x(x)           # Flatten y opcional conv
    total_forecast = 0.0
    nodos = [x0]                   # Nivel 0: 1 nodo
    
    for nivel in range(self.profundidad):
        fcs = []
        nuevos_nodos = []
        
        for nodo in nodos:
            # Procesar nodo con todas las ramas
            bcs, fcs_branch = self.block.forward_all_branches(nodo)
            fcs.extend(fcs_branch)  # Acumular forecasts
            
            # Crear hijos (uno por cada rama)
            for bc in bcs:
                nuevo_nodo = nodo - bc
                nuevos_nodos.append(nuevo_nodo)
        
        # Agregar forecasts de este nivel
        nivel_forecast = torch.stack(fcs).mean(dim=0)
        total_forecast += nivel_forecast
        
        # Pasar a siguiente nivel
        nodos = nuevos_nodos
    
    return total_forecast.unsqueeze(-1)  # (batch, H, 1)
```

**Ejemplo de expansi√≥n del √°rbol** (profundidad=3, num_ramas=2):
```
Nivel 0: 1 nodo  ‚Üí genera 2 forecasts
Nivel 1: 2 nodos ‚Üí generan 4 forecasts
Nivel 2: 4 nodos ‚Üí generan 8 forecasts
Nivel 3: 8 nodos ‚Üí generan 16 forecasts

Total forecasts = 2 + 4 + 8 + 16 = 30
Predicci√≥n final = promedio de los 30 forecasts
```

---

## üîß Detalles T√©cnicos Importantes

### 1. Por qu√© `forward_all_branches`?

**Problema original**:
```python
# ‚ùå INCORRECTO (bug original)
nuevos_nodos.extend([nodo - bc, nodo - bc])  # Duplicados id√©nticos!
```

**Soluci√≥n**:
```python
# ‚úÖ CORRECTO
bcs, fcs = self.block.forward_all_branches(nodo)
for bc in bcs:  # Cada rama genera un backcast diferente
    nuevos_nodos.append(nodo - bc)
```

**Por qu√© importa**:
- Cada rama tiene su propio gate ‚Üí selecciona diferentes features
- Backcasts diferentes ‚Üí nodos hijos diferentes
- Diversidad en el √°rbol ‚Üí mejor ensemble

### 2. Manejo de Dimensiones

```python
Input: (batch=32, L=96, D=7)
‚Üì _prep_x + flatten
(32, 672)  # 96 √ó 7
‚Üì MultiBranchBlock
backcasts: (32, 672)
forecasts: (32, H)
‚Üì Acumulaci√≥n por niveles
total_forecast: (32, H)
‚Üì unsqueeze
Output: (32, H, 1)
```

### 3. Conv1D para Covariables

**Motivaci√≥n**: Procesar las D=7 covariables temporales antes de flatten.

```python
Conv1d(in_channels=7, out_channels=7, kernel_size=1)
```

- **kernel_size=1**: Solo mezcla entre covariables, no temporal
- **in=out**: Mantiene n√∫mero de variables
- **Efecto**: Aprende combinaciones lineales de covariables √∫tiles

---

## üéõÔ∏è Configuraci√≥n de Hiperpar√°metros

### Valores Utilizados

```python
EPOCAS = 60
BATCH = 32
LR = 1e-4
SEED = 42

# Modelo
TD_OCULTO = 128        # hidden_gate y hidden_core
TD_PROF = 3            # profundidad_arbol
TD_RAMAS = 2           # num_ramas
MLP_DEPTH = 2          # Capas por MLP
DROPOUT = 0.10

# Optimizaci√≥n
USAR_AMP = True        # Mixed precision (bfloat16)
USAR_SCHEDULER = True  # ReduceLROnPlateau
FACTOR_SCHED = 0.5
PACIENCIA_SCHED = 3

# Early Stopping
USAR_EARLYSTOP = True
PACIENCIA_ES = 8
MIN_DELTA_ES = 1e-5
```

### Trade-offs

**Profundidad del √Årbol (TD_PROF)**:
- ‚Üë Profundidad ‚Üí M√°s forecasts, m√°s capacidad
- ‚Üë Profundidad ‚Üí M√°s nodos, m√°s c√≥mputo
- Profundidad=3: 2 + 4 + 8 = 14 forecasts promediados
- Profundidad=4: 2 + 4 + 8 + 16 = 30 forecasts promediados

**N√∫mero de Ramas (TD_RAMAS)**:
- ‚Üë Ramas ‚Üí M√°s diversidad en ensemble
- ‚Üë Ramas ‚Üí Crecimiento exponencial de nodos (K^depth)
- num_ramas=2 con depth=3: 8 nodos finales
- num_ramas=3 con depth=3: 27 nodos finales

---

## üìä Pipeline de Datos

### 1. Carga y Split

```python
# src/preprocesamiento.py
def cargar_ett(dataset, long_ventana, horizonte, col_objetivo="OT"):
    df = pd.read_csv(f"datos/ETT-small/{dataset}.csv")
    
    # Split temporal (sin shuffle)
    n = len(df)
    n_train = int(n * 0.70)  # 70% train
    n_val = int(n * 0.10)    # 10% val
    # Resto (20%) es test
    
    df_train = df[:n_train]
    df_val = df[n_train:n_train+n_val]
    df_test = df[n_train+n_val:]
    
    return ds_train, ds_val, ds_test, info
```

### 2. Normalizaci√≥n

```python
# Ajustar scalers SOLO en train
esc_x = StandardScaler().fit(df_train[features])
esc_y = StandardScaler().fit(df_train[["OT"]])

# Aplicar a todos los sets
X_train = esc_x.transform(df_train[features])
y_train = esc_y.transform(df_train[["OT"]])
# ... similar para val y test
```

**Importante**: Evita data leakage usando solo estad√≠sticas de train.

### 3. Ventanas Deslizantes

```python
class DatasetVentanasETT(Dataset):
    def __getitem__(self, i):
        L, H = self.long_ventana, self.horizonte
        
        x = self.X[i : i+L]         # (L, D) - entrada
        y = self.y[i+L : i+L+H]     # (H, 1) - target
        
        return torch.from_numpy(x), torch.from_numpy(y)
```

**Ejemplo** (L=96, H=24):
```
Index 0:  X = datos[0:96],    Y = datos[96:120]
Index 1:  X = datos[1:97],    Y = datos[97:121]
...
```

### 4. DataLoader Optimizado

```python
dl_train = DataLoader(
    ds_train,
    batch_size=32,
    shuffle=True,           # Solo en train
    num_workers=6,          # Carga paralela
    pin_memory=True,        # Optimizaci√≥n GPU
    persistent_workers=True,# Mantener workers vivos
    prefetch_factor=2       # Pre-cargar batches
)
```

---

## üèãÔ∏è Loop de Entrenamiento

### 1. Optimizaci√≥n

```python
optimizer = AdamW(
    model.parameters(),
    lr=1e-4,
    weight_decay=1e-2  # Regularizaci√≥n L2
)

scheduler = ReduceLROnPlateau(
    optimizer,
    mode='min',
    factor=0.5,        # LR ‚Üí LR/2
    patience=3,        # Esperar 3 √©pocas sin mejora
    min_lr=1e-6
)
```

### 2. Mixed Precision Training

```python
scaler = GradScaler(enabled=True)

for xb, yb in train_loader:
    with autocast(dtype=torch.bfloat16):  # Precisi√≥n reducida
        pred = model(xb)
        loss = mse_loss(pred, yb)
    
    scaler.scale(loss).backward()
    clip_grad_norm_(model.parameters(), 1.0)  # Gradient clipping
    scaler.step(optimizer)
    scaler.update()
```

**Beneficio**: ~40% m√°s r√°pido con p√©rdida m√≠nima de precisi√≥n.

### 3. Early Stopping

```python
class EarlyStopping:
    def step(self, val_loss):
        if val_loss < self.best - self.min_delta:
            self.best = val_loss
            self.counter = 0
            return False  # Continuar
        else:
            self.counter += 1
            return self.counter >= self.patience
```

### 4. Checkpointing

```python
if val_mse < best_val_mse:
    best_val_mse = val_mse
    torch.save({
        'modelo': model.state_dict(),
        'epoch': epoch,
        'val_mse': val_mse
    }, checkpoint_path)
```

**Estrategia**: Guardar solo el mejor modelo seg√∫n MSE de validaci√≥n.

---

## üìà Evaluaci√≥n

### M√©tricas Implementadas

```python
def evaluar_test(modelo, ds_test, device, esc_y, mape_original=True):
    for xb, yb in test_loader:
        pred = modelo(xb)
        
        # M√©tricas en escala normalizada
        mse = MSELoss()(pred, yb)
        mae = L1Loss()(pred, yb)
        rmse = sqrt(mse)
        r2 = 1 - SS_res/SS_tot
        
        # MAPE en escala original (desnormalizado)
        if mape_original:
            y_orig = esc_y.inverse_transform(yb)
            p_orig = esc_y.inverse_transform(pred)
            mape = mean(|y_orig - p_orig| / |y_orig|) * 100
```

**Por qu√© MAPE original?**
- MSE/MAE en escala normalizada son comparables entre horizontes
- MAPE necesita escala original para interpretabilidad (%)

---

## üé® Visualizaciones

### Gr√°ficas Generadas

1. **M√©tricas por √©poca**: MSE, MAE, RMSE, MAPE, R¬≤ en validaci√≥n
2. **Learning rate**: Evoluci√≥n con scheduler
3. **Velocidad**: Iteraciones/s y muestras/s
4. **Serie temporal**: √öltima ventana de test (historial + predicci√≥n vs real)
5. **Comparativas**: M√©tricas vs horizonte de predicci√≥n

---

## üîç Debugging Tips

### Verificar Shapes

```python
print(f"Input: {x.shape}")           # (32, 96, 7)
print(f"After prep: {x0.shape}")     # (32, 672)
print(f"Backcast: {bc.shape}")       # (32, 672)
print(f"Forecast: {fc.shape}")       # (32, H)
print(f"Output: {y.shape}")          # (32, H, 1)
```

### Verificar Gradientes

```python
for name, param in model.named_parameters():
    if param.grad is not None:
        print(f"{name}: grad_norm={param.grad.norm().item():.4f}")
```

### Verificar Diversidad de Ramas

```python
bcs, fcs = model.block.forward_all_branches(x0)
print(f"Backcast 0: {bcs[0][:5]}")
print(f"Backcast 1: {bcs[1][:5]}")
# Deben ser diferentes!
```

---

## üìù Clean Code Practices

1. **Sin comentarios**: El c√≥digo se auto-documenta con nombres claros
2. **Type hints**: `def func(x: torch.Tensor) -> torch.Tensor`
3. **Separaci√≥n de concerns**: Cada m√≥dulo tiene una responsabilidad
4. **DRY**: No repetir l√≥gica (ej. `forward_all_branches`)
5. **Configuraci√≥n centralizada**: Todos los hiperpar√°metros en `experimentos.py`

