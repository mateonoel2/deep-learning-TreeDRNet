# MetodologÃ­a TreeDRNet

> **Tiempo de presentaciÃ³n**: 5 minutos  
> **Objetivo**: Explicar el approach del paper y fundamentos teÃ³ricos

## ğŸ“‘ Ãndice de Contenidos (Timing Sugerido)

1. **MotivaciÃ³n y Problema** (1 min) - Por quÃ© TreeDRNet
2. **Arquitectura Principal** (2 min) - Los 4 componentes clave
3. **Fundamentos TeÃ³ricos** (1 min) - Robust regression + Kolmogorov-Arnold
4. **Ventajas y Resultados** (1 min) - Por quÃ© es mejor que SOTA

---

## ğŸ“„ Paper Original

**TreeDRNet: A Robust Deep Model for Long Term Time Series Forecasting**  
Zhou, T., Zhu, J., Wang, X., Ma, Z., Wen, Q., Sun, L., & Jin, R. (2022)  
[arXiv:2206.12106](https://arxiv.org/abs/2206.12106)

---

## ğŸ¯ MotivaciÃ³n

### Problema a Resolver

**Long-term Time Series Forecasting**: Predecir mÃºltiples pasos futuros (ej. 720 horizones) a partir de secuencias largas de entrada.

### Limitaciones de MÃ©todos Existentes

1. **Transformers**:
   - Alta complejidad computacional: O(nÂ²)
   - Deterioro de performance con secuencias largas
   - Tendencia al overfitting con modelos muy complejos

2. **Modelos tradicionales**:
   - Incapaces de capturar patrones complejos no lineales
   - Sensibles a ruido y outliers
   - Pobre generalizaciÃ³n

---

## ğŸ—ï¸ Arquitectura TreeDRNet

### 1. Doubly Residual (DRes) Structure

Inspirado en **robust regression**, cada bloque DRes produce:

```
Input x â†’ MLP â†’ {backcast, forecast}
```

- **Backcast (bc)**: ReconstrucciÃ³n del input para capturar patrones histÃ³ricos
- **Forecast (fc)**: PredicciÃ³n del horizonte futuro

**Ventaja**: La separaciÃ³n backcast/forecast mejora la estabilidad y robustez de las predicciones.

```python
residual = x - backcast  # Lo que no se capturÃ³
prediction = Î£ forecasts  # AgregaciÃ³n de predicciones
```

### 2. Gating Mechanism (Feature Selection)

Cada rama tiene un **gate network** que aprende a seleccionar features relevantes:

```
gate = Ïƒ(MLP(x))           # Ïƒ = sigmoid
x_selected = x âŠ™ gate      # âŠ™ = element-wise multiplication
```

**Ventaja**: 
- SelecciÃ³n automÃ¡tica de features importantes
- Diferentes ramas seleccionan diferentes aspectos
- Reduce ruido y mejora robustez

### 3. Tree Structure (Hierarchical Ensemble)

Basado en el **Teorema de Kolmogorov-Arnold**: funciones multivariadas pueden descomponerse jerÃ¡rquicamente.

```
Nivel 0:           [xâ‚€]
                   â†™  â†˜
Nivel 1:      [xâ‚€-bcâ‚] [xâ‚€-bcâ‚‚]
              â†™ â†˜      â†™ â†˜
Nivel 2:   [...]  [...] [...] [...]
```

Cada nivel:
- **Procesa** el nodo actual â†’ genera forecast
- **Divide** el nodo en K ramas (hijos) usando diferentes backcasts
- **Acumula** forecasts para predicciÃ³n final

**Ventaja**:
- Ensemble de mÃºltiples "expertos" (cada rama)
- RepresentaciÃ³n jerÃ¡rquica de patrones
- Diversidad entre ramas mejora robustez

### 4. Multi-Branch per Node

En cada nodo, K ramas paralelas procesan el mismo input:

```
       x_node
     /   |   \
   brâ‚  brâ‚‚  brâ‚ƒ
    |    |    |
  bcâ‚  bcâ‚‚  bcâ‚ƒ  â†’ K diferentes backcasts
  fcâ‚  fcâ‚‚  fcâ‚ƒ  â†’ K diferentes forecasts
```

**Ventaja**: Ensemble interno en cada nodo aumenta diversidad.

---

## ğŸ”¬ Fundamentos TeÃ³ricos

### 1. Robust Regression

TreeDRNet adapta ideas de regresiÃ³n robusta:
- **Residuos mÃºltiples**: Cada nivel refina el residuo anterior
- **M-estimators**: MÃºltiples ramas actÃºan como estimadores robustos
- **Resistencia a outliers**: Ensemble reduce impacto de predicciones anÃ³malas

### 2. Kolmogorov-Arnold Representation Theorem

> Toda funciÃ³n continua multivariada f(xâ‚, ..., xâ‚™) puede representarse como:
> 
> f(x) = Î£áµ¢ Ï†áµ¢(Î£â±¼ Ïˆáµ¢â±¼(xâ±¼))

TreeDRNet implementa esto mediante:
- **Outer sum** (Î£áµ¢): Suma de forecasts a travÃ©s de niveles
- **Inner sum** (Î£â±¼): CombinaciÃ³n de features vÃ­a gates y MLPs
- **Hierarchy**: Estructura de Ã¡rbol = descomposiciÃ³n funcional

### 3. Ensemble Learning

- **Bagging effect**: MÃºltiples ramas â†’ reducciÃ³n de varianza
- **Boosting effect**: MÃºltiples niveles â†’ refinamiento secuencial
- **Diversidad**: Gates diferentes â†’ especializaciones distintas

---

## ğŸ“Š Algoritmo de Forward Pass

```
Input: x âˆˆ â„^(LÃ—D)  # L=longitud ventana, D=dimensiones

1. Preprocesar:
   xâ‚€ = Conv1D(x) si usar_conv
   xâ‚€ = flatten(xâ‚€)  # Shape: (LÃ—D,)

2. Inicializar:
   nodos = [xâ‚€]
   total_forecast = 0

3. Para cada nivel l en Ã¡rbol:
   nuevos_nodos = []
   level_forecasts = []
   
   Para cada nodo en nodos:
      Para cada rama k en [1..K]:
         gate_k = sigmoid(MLP_gate_k(nodo))
         x_sel = nodo âŠ™ gate_k
         bc_k = MLP_backcast_k(x_sel)
         fc_k = MLP_forecast_k(x_sel)
         
         level_forecasts.append(fc_k)
         nuevos_nodos.append(nodo - bc_k)
   
   total_forecast += mean(level_forecasts)
   nodos = nuevos_nodos

4. Return: total_forecast
```

---

## ğŸ’¡ Ventajas Clave del MÃ©todo

### 1. Eficiencia Computacional
- **Solo MLPs**: O(n) vs O(nÂ²) de attention
- **10Ã— mÃ¡s rÃ¡pido** que Transformers segÃºn el paper
- Paralelizable en GPU de forma natural

### 2. Robustez
- **Ensemble mÃºltiple**: Reduce overfitting
- **Doubly residual**: Maneja mejor patrones complejos
- **Gating**: Filtra ruido automÃ¡ticamente

### 3. Long-term Forecasting
- **Estructura jerÃ¡rquica**: Captura patrones multi-escala
- **Refinamiento progresivo**: Cada nivel mejora la predicciÃ³n
- **RepresentaciÃ³n rica**: Ãrbol aumenta capacidad expresiva

### 4. Interpretabilidad (parcial)
- **Gates**: Muestran quÃ© features son importantes
- **Niveles**: Cada nivel captura diferentes aspectos
- **Ramas**: Especializaciones visualizables

---

## ğŸ“ˆ Resultados Reportados en el Paper

### Performance vs State-of-the-Art

En datasets de benchmark (ETT, Weather, Electricity):
- **20-40% reducciÃ³n** en MSE/MAE comparado con Informer, Autoformer, FEDformer
- **Mejor robustez** en horizontes largos (H=720)
- **10Ã— mÃ¡s rÃ¡pido** en entrenamiento e inferencia

### Ablation Studies

| Componente Removido | Impacto en MSE |
|-------------------|----------------|
| Sin Gating | +15-25% |
| Sin Tree (depth=1) | +10-18% |
| Sin DRes | +12-20% |
| Sin Multi-branch | +8-15% |

**ConclusiÃ³n**: Todos los componentes son importantes para el performance.

---

## ğŸ›ï¸ HiperparÃ¡metros Clave

| ParÃ¡metro | DescripciÃ³n | Valor TÃ­pico |
|-----------|-------------|--------------|
| `tree_depth` | Profundidad del Ã¡rbol | 2-4 |
| `num_branches` | Ramas por nodo | 2-4 |
| `hidden_dim` | DimensiÃ³n oculta MLPs | 128-256 |
| `mlp_depth` | Capas en cada MLP | 2-3 |
| `dropout` | RegularizaciÃ³n | 0.1-0.2 |

**Trade-offs**:
- â†‘ `tree_depth`: MÃ¡s capacidad pero mÃ¡s parÃ¡metros
- â†‘ `num_branches`: MÃ¡s diversidad pero mÃ¡s costo computacional
- â†‘ `hidden_dim`: MÃ¡s expresividad pero mayor riesgo de overfitting

---

## ğŸ”— ComparaciÃ³n con Otros MÃ©todos

| MÃ©todo | Complejidad | Long-term | Robustez | Eficiencia |
|--------|-------------|-----------|----------|------------|
| ARIMA | O(n) | âŒ Pobre | âš ï¸ Media | âœ… Alta |
| LSTM | O(n) | âš ï¸ Media | âš ï¸ Media | âœ… Alta |
| Transformer | O(nÂ²) | âœ… Buena | âš ï¸ Media | âŒ Baja |
| **TreeDRNet** | **O(n)** | **âœ… Muy Buena** | **âœ… Alta** | **âœ… Alta** |

---

## ğŸ“š Conceptos Clave para Recordar

1. **Doubly Residual**: Backcast (reconstrucciÃ³n) + Forecast (predicciÃ³n)
2. **Gating**: SelecciÃ³n automÃ¡tica de features relevantes
3. **Tree Structure**: Ensemble jerÃ¡rquico de modelos
4. **Multi-branch**: MÃºltiples "expertos" por nodo
5. **MLP-only**: Eficiencia computacional sin sacrificar performance

---

## ğŸ“ Aplicabilidad

TreeDRNet es especialmente Ãºtil para:

âœ… **Series largas** (L > 96)  
âœ… **Horizontes largos** (H > 96)  
âœ… **Datos ruidosos** (robustez importante)  
âœ… **Recursos limitados** (eficiencia crÃ­tica)  
âœ… **Multi-variable** (covariables disponibles)

âŒ Menos ideal para:
- Series muy cortas (< 50 puntos)
- Horizontes muy cortos (H < 24)
- Datos univariados simples

